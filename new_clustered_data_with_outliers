import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from tslearn.clustering import TimeSeriesKMeans
from tslearn.preprocessing import TimeSeriesScalerMinMax
from sklearn.preprocessing import MinMaxScaler
from scipy.spatial.distance import cdist

# Load Data
data = pd.read_csv('synthetic_data.csv')

# Feature Engineering
def feature_engineering(data):
    # Calculate moving averages
    data['moving_avg_3'] = data[[f'PrevMonConsumption_{i}' for i in range(1, 4)]].mean(axis=1)
    data['moving_avg_6'] = data[[f'PrevMonConsumption_{i}' for i in range(1, 7)]].mean(axis=1)

    # Calculate trend (slope)
    x = np.arange(1, 12).reshape(-1, 1)
    y_cols = [f'PrevMonConsumption_{i}' for i in range(1, 12)]
    slopes = np.apply_along_axis(
        lambda y: LinearRegression().fit(x, y).coef_[0],
        axis=1,
        arr=data[y_cols].values
    )
    data['trend_slope'] = slopes

    # Calculate lag differences
    for i in range(1, 11):
        data[f'lag_diff_{i}'] = data[f'PrevMonConsumption_{i}'] - data[f'PrevMonConsumption_{i+1}']

    return data

# Normalize Time Series for Clustering
def normalize_time_series(data, cols, scaler_range=(0, 1)):
    scaler = TimeSeriesScalerMinMax(value_range=scaler_range)
    scaled_data = scaler.fit_transform(data[cols].values)
    return scaled_data

# Time Series Clustering
def perform_clustering(time_series_data, n_clusters=3):
    kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric="dtw", random_state=42)
    labels = kmeans.fit_predict(time_series_data)
    return kmeans, labels

def merge_small_clusters(data, cluster_labels, min_cluster_size=5):
    unique_clusters, cluster_counts = np.unique(cluster_labels, return_counts=True)
    small_clusters = unique_clusters[cluster_counts < min_cluster_size]
    
    if len(small_clusters) == 0:
        return cluster_labels
    
    # Calculate distances between cluster centroids
    centroids = kmeans.cluster_centers_
    distances = cdist(centroids, centroids)
    
    # Merge small clusters into the nearest larger cluster
    for small_cluster in small_clusters:
        # Find the nearest larger cluster
        nearest_cluster = np.argmin([distances[small_cluster, i] if cluster_counts[i] >= min_cluster_size else np.inf for i in range(len(unique_clusters))])
        cluster_labels[cluster_labels == small_cluster] = nearest_cluster
    
    return cluster_labels

def plot_time_series_by_cluster_single_canvas(data, cluster_centroids, time_series_cols, n_clusters):
    # Normalize data for plotting
    scaler = MinMaxScaler()
    normalized_data = scaler.fit_transform(data[time_series_cols])

    # Create a figure with subplots for each cluster
    fig, axes = plt.subplots(n_clusters, 1, figsize=(12, 6 * n_clusters), sharex=True, sharey=True)
    
    # If there's only one cluster, axes will not be an array, so we wrap it in a list
    if n_clusters == 1:
        axes = [axes]

    # Plot each cluster in its own subplot
    for cluster, ax in enumerate(axes):
        # Filter data for the current cluster
        cluster_data = data[data['cluster_label'] == cluster]
        normalized_cluster_data = normalized_data[data['cluster_label'] == cluster]
        
        # Plot individual time series in the cluster
        for ts in normalized_cluster_data:
            ax.plot(range(1, len(time_series_cols) + 1), ts, color="gray", alpha=0.2, label='_nolegend_')
        
        # Plot the cluster centroid
        centroid = cluster_centroids[cluster]
        ax.plot(range(1, centroid.shape[0] + 1), centroid.flatten(), linewidth=2.5, label=f"Cluster {cluster} Centroid")
        
        # Add labels and titles
        ax.set_xlabel("Month")
        ax.set_ylabel("Normalized Consumption")
        ax.set_title(f"Cluster {cluster} - Normalized Time Series and Centroid")
        ax.legend()
        ax.grid()

    # Adjust layout for better spacing
    plt.tight_layout()
    plt.show()

from sklearn.decomposition import PCA

def plot_clusters_in_2d(data, cluster_labels, n_clusters, random_state=42):
    """
    Plot clusters in 2D using PCA for dimensionality reduction.
    """
    # Perform PCA dimensionality reduction
    pca = PCA(n_components=2, random_state=random_state)
    pca_results = pca.fit_transform(data)

    # Create a DataFrame for the PCA results
    pca_df = pd.DataFrame(pca_results, columns=['PCA1', 'PCA2'])
    pca_df['cluster_label'] = cluster_labels

    # Plot the clusters
    plt.figure(figsize=(10, 8))
    for cluster in range(n_clusters):
        cluster_data = pca_df[pca_df['cluster_label'] == cluster]
        plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], label=f'Cluster {cluster}', alpha=0.6)

    # Highlight outliers if available
    if 'is_outlier' in data.columns:
        outliers = pca_df[data['is_outlier']]
        plt.scatter(outliers['PCA1'], outliers['PCA2'], color='red', marker='x', s=100, label='Outliers')

    # Add labels and legend
    plt.title('2D Cluster Visualization using PCA')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.legend()
    plt.grid()
    plt.show()

# Main Execution
data = feature_engineering(data)

# Time Series Columns
time_series_cols = [f'PrevMonConsumption_{i}' for i in range(1, 12)]
scaled_time_series = normalize_time_series(data, time_series_cols)

# Clustering
n_clusters = 10
kmeans, cluster_labels = perform_clustering(scaled_time_series, n_clusters)

# Merge small clusters
cluster_labels = merge_small_clusters(data, cluster_labels, min_cluster_size=5)
data['cluster_label'] = cluster_labels

# Update the number of clusters after merging
n_clusters = len(np.unique(cluster_labels))

# Regression Features
features = time_series_cols + ['moving_avg_3', 'moving_avg_6', 'trend_slope'] + [f'lag_diff_{i}' for i in range(1, 11)] + ['cluster_label']
X = data[features]
y = data['ConsumptionValue']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Predictions and Residuals
data['predicted_consumption'] = model.predict(X)
data['residual'] = data['ConsumptionValue'] - data['predicted_consumption']

# Outlier Detection within Clusters
threshold = 2 * data['residual'].std()
data['is_outlier'] = data['residual'].abs() > threshold

# Check if all users in a cluster are outliers
for cluster in range(n_clusters):
    cluster_data = data[data['cluster_label'] == cluster]
    if cluster_data['is_outlier'].all():
        data.loc[data['cluster_label'] == cluster, 'is_outlier'] = False

# Save Results
outliers = data[['BillNumber', 'ConsumptionValue', 'predicted_consumption', 'residual', 'is_outlier', 'cluster_label']]
outliers.to_csv('outliers_with_clusters.csv', index=False)

# Model Evaluation
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae:.2f}")
print("Outliers with clusters saved to 'outliers_with_clusters.csv'.")

# Plot Results by Cluster
plot_time_series_by_cluster_single_canvas(data, kmeans.cluster_centers_, time_series_cols, n_clusters)
plot_clusters_in_2d(data, cluster_labels, n_clusters)